{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02 – Feature Selection\n",
        "\n",
        "**Pipeline stages covered:**\n",
        "1. Load cleaned dataset from Notebook 01\n",
        "2. Stratified train / test split (80 / 20) — **the authoritative split**\n",
        "3. Fit MinMaxScaler **on training data only** (no leakage)\n",
        "4. Random Forest feature selection (Gini impurity, top 20)\n",
        "5. Visualize feature importances & correlation heatmap\n",
        "6. Persist: scaler, split indices, selected features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "from utils.preprocessing    import (\n",
        "    split_features_labels,\n",
        "    fit_scaler,\n",
        "    transform_features,\n",
        "    fit_label_encoder,\n",
        "    encode_labels,\n",
        "    get_label_mapping,\n",
        ")\n",
        "from utils.feature_selection import select_features_rf, save_selected_features\n",
        "from utils.visualization     import (\n",
        "    plot_feature_importance,\n",
        "    plot_all_feature_importance,\n",
        "    plot_correlation_heatmap,\n",
        "    plot_feature_distributions,\n",
        ")\n",
        "\n",
        "print('✓ Imports OK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Configuration ──────────────────────────────────────────────────────────\n",
        "DATA_DIR      = '../data/processed'\n",
        "OUTPUT_DIR    = '../data/processed'\n",
        "MODEL_DIR     = '../model'\n",
        "LABEL_COL     = 'Label'\n",
        "N_FEATURES    = 20       # top-K features to select\n",
        "TEST_SIZE     = 0.20\n",
        "RANDOM_STATE  = 42\n",
        "RF_ESTIMATORS = 100\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 · Load Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_parquet(f'{DATA_DIR}/cleaned_dataset.parquet')\n",
        "print(f'Loaded dataset: {df.shape[0]:,} rows × {df.shape[1]} cols')\n",
        "\n",
        "X, y, feature_names = split_features_labels(df, label_col=LABEL_COL)\n",
        "print(f'Features: {len(feature_names)}  |  Classes: {y.nunique()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 · Stratified Train / Test Split (80 / 20)\n",
        "\n",
        "> This is the **canonical** split. Both the scaler and the Random Forest\n",
        "> are fitted **only on `X_train`** to prevent data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "print(f'Training samples  : {len(X_train):,}  ({len(X_train)/len(X)*100:.1f}%)')\n",
        "print(f'Test samples      : {len(X_test):,}  ({len(X_test)/len(X)*100:.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualise split class balance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=120)\n",
        "\n",
        "train_counts = y_train.value_counts().sort_index()\n",
        "test_counts  = y_test.value_counts().sort_index()\n",
        "\n",
        "colors = plt.cm.tab20.colors\n",
        "ax1.barh(train_counts.index, train_counts.values, color=colors[:len(train_counts)])\n",
        "ax1.set_title('Training Split – Class Distribution', fontsize=11, fontweight='bold')\n",
        "ax1.set_xlabel('Count')\n",
        "\n",
        "ax2.barh(test_counts.index, test_counts.values, color=colors[:len(test_counts)])\n",
        "ax2.set_title('Test Split – Class Distribution', fontsize=11, fontweight='bold')\n",
        "ax2.set_xlabel('Count')\n",
        "\n",
        "plt.suptitle('Class Distribution: Train vs Test Split', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "fig.savefig(f'{OUTPUT_DIR}/fig_07_split_dist.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 · Label Encoding (One-Hot)\n",
        "\n",
        "Encoder fitted **only on `y_train`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = fit_label_encoder(y_train)\n",
        "y_train_enc   = encode_labels(label_encoder, y_train)\n",
        "y_test_enc    = encode_labels(label_encoder, y_test)\n",
        "\n",
        "class_names = label_encoder.classes_.tolist()\n",
        "print(f'Classes: {class_names}')\n",
        "print(f'y_train_enc shape: {y_train_enc.shape}')\n",
        "print(f'y_test_enc  shape: {y_test_enc.shape}')\n",
        "\n",
        "# Show encoded mapping\n",
        "mapping = get_label_mapping(label_encoder)\n",
        "print('\\nOne-hot mapping:')\n",
        "for cls, vec in mapping.items():\n",
        "    print(f'  {cls:<30} → {vec}')\n",
        "\n",
        "# Save encoder\n",
        "joblib.dump(label_encoder, f'{MODEL_DIR}/label_encoder.pkl')\n",
        "print(f'\\n✓ label_encoder saved → {MODEL_DIR}/label_encoder.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 · Feature Scaling (MinMax 0–1)\n",
        "\n",
        "Scaler fitted **only on `X_train`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler      = fit_scaler(X_train)\n",
        "X_train_sc  = transform_features(scaler, X_train)\n",
        "X_test_sc   = transform_features(scaler, X_test)\n",
        "\n",
        "print(f'Scaler fitted on {len(X_train):,} training samples.')\n",
        "print(f'X_train_sc range: [{X_train_sc.min():.4f}, {X_train_sc.max():.4f}]')\n",
        "print(f'X_test_sc  range: [{X_test_sc.min():.4f}, {X_test_sc.max():.4f}]')\n",
        "\n",
        "# Persist scaler\n",
        "joblib.dump(scaler, f'{MODEL_DIR}/scaler.pkl')\n",
        "print(f'\\n✓ Scaler saved → {MODEL_DIR}/scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distributions before vs after scaling\n",
        "fig = plot_feature_distributions(\n",
        "    X_raw=X_train,\n",
        "    X_scaled=X_train_sc,\n",
        "    feature_names=feature_names,\n",
        "    n_features=6,\n",
        "    title='Feature Distributions Before vs After MinMax Scaling (Train Set)',\n",
        ")\n",
        "fig.savefig(f'{OUTPUT_DIR}/fig_08_scaling_dist.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 · Random Forest Feature Selection (Top 20)\n",
        "\n",
        "The RF is trained **only on `X_train_sc`** and `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features, importances_df, rf_model = select_features_rf(\n",
        "    X_train=X_train_sc,\n",
        "    y_train_raw=y_train,\n",
        "    feature_names=feature_names,\n",
        "    n_features=N_FEATURES,\n",
        "    n_estimators=RF_ESTIMATORS,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(f'\\nSelected {len(selected_features)} features:')\n",
        "for i, f in enumerate(selected_features, 1):\n",
        "    print(f'  {i:>2}. {f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 · Visualize Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All features – top highlighted\n",
        "fig = plot_all_feature_importance(importances_df, n_top=N_FEATURES)\n",
        "fig.savefig(f'{OUTPUT_DIR}/fig_09_all_importances.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top-20 features detailed view\n",
        "fig = plot_feature_importance(importances_df, n_features=N_FEATURES,\n",
        "                               title=f'Top {N_FEATURES} Features by Gini Importance')\n",
        "fig.savefig(f'{OUTPUT_DIR}/fig_10_top20_importance.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap of the top-20 features (training set only)\n",
        "train_df_top20 = pd.DataFrame(X_train_sc, columns=feature_names)[selected_features]\n",
        "fig = plot_correlation_heatmap(train_df_top20,\n",
        "                                title='Correlation Heatmap – Top 20 Features (Train Set)')\n",
        "fig.savefig(f'{OUTPUT_DIR}/fig_11_correlation.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 · Persist Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save selected features list\n",
        "save_selected_features(selected_features, f'{MODEL_DIR}/selected_features.json')\n",
        "\n",
        "# Save processed splits as compressed numpy arrays (indexed by original position)\n",
        "feat_idx = [feature_names.index(f) for f in selected_features]\n",
        "\n",
        "np.save(f'{OUTPUT_DIR}/X_train_sc.npy', X_train_sc[:, feat_idx])\n",
        "np.save(f'{OUTPUT_DIR}/X_test_sc.npy',  X_test_sc[:, feat_idx])\n",
        "np.save(f'{OUTPUT_DIR}/y_train_enc.npy', y_train_enc)\n",
        "np.save(f'{OUTPUT_DIR}/y_test_enc.npy',  y_test_enc)\n",
        "\n",
        "# Save raw label arrays for metrics computation later\n",
        "y_train.reset_index(drop=True).to_json(f'{OUTPUT_DIR}/y_train_raw.json')\n",
        "y_test.reset_index(drop=True).to_json(f'{OUTPUT_DIR}/y_test_raw.json')\n",
        "\n",
        "# Save metadata\n",
        "meta = {\n",
        "    'n_features' : N_FEATURES,\n",
        "    'n_classes'  : len(class_names),\n",
        "    'class_names': class_names,\n",
        "    'selected_features': selected_features,\n",
        "    'test_size'  : TEST_SIZE,\n",
        "    'random_state': RANDOM_STATE,\n",
        "}\n",
        "with open(f'{OUTPUT_DIR}/metadata.json', 'w') as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print('✓ All artifacts saved:')\n",
        "print(f'   {MODEL_DIR}/selected_features.json')\n",
        "print(f'   {MODEL_DIR}/scaler.pkl')\n",
        "print(f'   {MODEL_DIR}/label_encoder.pkl')\n",
        "print(f'   {OUTPUT_DIR}/X_train_sc.npy / X_test_sc.npy')\n",
        "print(f'   {OUTPUT_DIR}/y_train_enc.npy / y_test_enc.npy')\n",
        "print(f'   {OUTPUT_DIR}/metadata.json')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
