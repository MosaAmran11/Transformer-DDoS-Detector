{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03 – Transformer Model Training & Evaluation\n",
        "\n",
        "**Pipeline stages covered:**\n",
        "1. Load pre-processed splits from Notebook 02\n",
        "2. Build PyTorch DataLoaders\n",
        "3. Instantiate Transformer classifier\n",
        "4. Training loop with live progress\n",
        "5. Loss & accuracy curves\n",
        "6. Evaluation: Accuracy · Precision · Recall · F1-Score\n",
        "7. Confusion matrix (raw + normalized)\n",
        "8. Per-class metrics bar chart & radar chart\n",
        "9. ROC curves (one-vs-rest)\n",
        "10. Save model + all artifacts to `model/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from utils.model_utils import (\n",
        "    TransformerClassifier,\n",
        "    make_dataloaders,\n",
        "    train_model,\n",
        "    eval_epoch,\n",
        "    compute_metrics,\n",
        "    save_model,\n",
        ")\n",
        "from utils.visualization import (\n",
        "    plot_training_history,\n",
        "    plot_confusion_matrix,\n",
        "    plot_evaluation_metrics,\n",
        "    plot_metrics_radar,\n",
        "    plot_roc_curves,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'✓ Imports OK  |  Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Configuration ──────────────────────────────────────────────────────────\n",
        "DATA_DIR   = '../data/processed'\n",
        "MODEL_DIR  = '../model'\n",
        "\n",
        "# Transformer hyperparameters\n",
        "D_MODEL        = 128\n",
        "N_HEADS        = 4\n",
        "N_LAYERS       = 2\n",
        "DIM_FEEDFORWARD= 256\n",
        "DROPOUT        = 0.1\n",
        "\n",
        "# Training hyperparameters\n",
        "N_EPOCHS       = 30\n",
        "BATCH_SIZE     = 512\n",
        "LEARNING_RATE  = 1e-3\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "print('Configuration set.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 · Load Pre-processed Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'{DATA_DIR}/metadata.json') as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "class_names       = meta['class_names']\n",
        "selected_features = meta['selected_features']\n",
        "n_classes         = meta['n_classes']\n",
        "n_features        = meta['n_features']\n",
        "\n",
        "X_train = np.load(f'{DATA_DIR}/X_train_sc.npy')\n",
        "X_test  = np.load(f'{DATA_DIR}/X_test_sc.npy')\n",
        "y_train = np.load(f'{DATA_DIR}/y_train_enc.npy')\n",
        "y_test  = np.load(f'{DATA_DIR}/y_test_enc.npy')\n",
        "\n",
        "y_train_raw = pd.read_json(f'{DATA_DIR}/y_train_raw.json', typ='series')\n",
        "y_test_raw  = pd.read_json(f'{DATA_DIR}/y_test_raw.json',  typ='series')\n",
        "\n",
        "scaler        = joblib.load(f'{MODEL_DIR}/scaler.pkl')\n",
        "label_encoder = joblib.load(f'{MODEL_DIR}/label_encoder.pkl')\n",
        "\n",
        "print(f'Classes ({n_classes}): {class_names}')\n",
        "print(f'Selected features ({n_features}): {selected_features}')\n",
        "print(f'X_train: {X_train.shape}  y_train: {y_train.shape}')\n",
        "print(f'X_test : {X_test.shape}   y_test : {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 · Build DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader, test_loader = make_dataloaders(\n",
        "    X_train, y_train,\n",
        "    X_test,  y_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "print(f'Train batches: {len(train_loader)}  |  Test batches: {len(test_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 · Instantiate Transformer Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = TransformerClassifier(\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    d_model=D_MODEL,\n",
        "    n_heads=N_HEADS,\n",
        "    n_layers=N_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(model)\n",
        "print(f'\\nTrainable parameters: {total_params:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 · Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 · Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plot_training_history(history, title='Transformer DDoS Classifier – Training History')\n",
        "fig.savefig(f'{DATA_DIR}/fig_12_training_history.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "best_train_acc = max(history['train_acc'])\n",
        "best_val_acc   = max(history['val_acc'])\n",
        "best_val_loss  = min(history['val_loss'])\n",
        "print(f'Best train acc : {best_train_acc:.4f}')\n",
        "print(f'Best val acc   : {best_val_acc:.4f}')\n",
        "print(f'Best val loss  : {best_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 · Final Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss, test_acc, y_pred_idx, y_true_idx, y_probs = eval_epoch(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(f'Test Loss     : {test_loss:.4f}')\n",
        "print(f'Test Accuracy : {test_acc:.4f}  ({test_acc*100:.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full classification report\n",
        "metrics = compute_metrics(y_true_idx, y_pred_idx, class_names)\n",
        "\n",
        "print('=== Classification Report ===')\n",
        "print(classification_report(\n",
        "    y_true_idx, y_pred_idx,\n",
        "    target_names=class_names,\n",
        "    digits=4,\n",
        "    zero_division=0,\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 · Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = metrics['confusion_matrix']\n",
        "\n",
        "# Raw counts\n",
        "fig = plot_confusion_matrix(cm, class_names,\n",
        "                             title='Confusion Matrix (Raw Counts)',\n",
        "                             normalize=False)\n",
        "fig.savefig(f'{DATA_DIR}/fig_13_cm_raw.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalized (recall per class)\n",
        "fig = plot_confusion_matrix(cm, class_names,\n",
        "                             title='Confusion Matrix (Normalized – Recall per Class)',\n",
        "                             normalize=True)\n",
        "fig.savefig(f'{DATA_DIR}/fig_14_cm_normalized.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 · Evaluation Metrics – Bar Chart & Radar Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "skip_keys = {'accuracy', 'macro avg', 'weighted avg', 'confusion_matrix'}\n",
        "metrics_table = {\n",
        "    cls: {k: round(v, 4) for k, v in vals.items() if k != 'support'}\n",
        "    for cls, vals in metrics.items()\n",
        "    if cls not in skip_keys\n",
        "}\n",
        "metrics_table['accuracy'] = round(metrics['accuracy'], 4)\n",
        "metrics_table['macro avg']    = {k: round(v, 4) for k, v in metrics['macro avg'].items()    if k != 'support'}\n",
        "metrics_table['weighted avg'] = {k: round(v, 4) for k, v in metrics['weighted avg'].items() if k != 'support'}\n",
        "\n",
        "summary_df = pd.DataFrame(metrics_table).T\n",
        "print('=== Metrics Summary ===')\n",
        "display(summary_df.style.background_gradient(cmap='RdYlGn', axis=None, vmin=0, vmax=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grouped bar chart: Precision, Recall, F1 per class\n",
        "fig = plot_evaluation_metrics(\n",
        "    metrics,\n",
        "    title='Per-Class Metrics: Precision · Recall · F1-Score  |  Overall Accuracy'\n",
        ")\n",
        "fig.savefig(f'{DATA_DIR}/fig_15_metrics_bar.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Radar chart\n",
        "fig = plot_metrics_radar(metrics, title='Per-Class Precision / Recall / F1 Radar')\n",
        "fig.savefig(f'{DATA_DIR}/fig_16_metrics_radar.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9 · ROC Curves (One-vs-Rest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plot_roc_curves(\n",
        "    y_true_onehot=y_test,\n",
        "    y_prob=y_probs,\n",
        "    class_names=class_names,\n",
        "    title='ROC Curves – Transformer DDoS Classifier (One-vs-Rest)',\n",
        ")\n",
        "fig.savefig(f'{DATA_DIR}/fig_17_roc_curves.png', dpi=120, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10 · Per-Epoch Accuracy & Loss Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_df = pd.DataFrame(history)\n",
        "history_df.index = history_df.index + 1\n",
        "history_df.index.name = 'Epoch'\n",
        "\n",
        "# Highlight best epochs\n",
        "display(\n",
        "    history_df.style\n",
        "    .highlight_max(subset=['train_acc', 'val_acc'], color='#C8E6C9')\n",
        "    .highlight_min(subset=['train_loss', 'val_loss'], color='#BBDEFB')\n",
        "    .format('{:.4f}')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11 · Save Model & Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_model(\n",
        "    model=model,\n",
        "    scaler=scaler,\n",
        "    label_encoder=label_encoder,\n",
        "    selected_features=selected_features,\n",
        "    model_dir=MODEL_DIR,\n",
        ")\n",
        "\n",
        "print('\\n=== Saved Artifacts ===')\n",
        "for root, dirs, files in os.walk(MODEL_DIR):\n",
        "    for fname in files:\n",
        "        fpath = os.path.join(root, fname)\n",
        "        size  = os.path.getsize(fpath)\n",
        "        print(f'  {fname:<35}  {size/1024:.1f} KB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print('=' * 55)\n",
        "print('  FINAL MODEL SUMMARY')\n",
        "print('=' * 55)\n",
        "print(f'  Architecture  : Transformer ({N_LAYERS} layers, {N_HEADS} heads, d={D_MODEL})')\n",
        "print(f'  Input features: {n_features}')\n",
        "print(f'  Output classes: {n_classes}')\n",
        "print(f'  Epochs trained: {N_EPOCHS}')\n",
        "print(f'  Test Accuracy : {test_acc*100:.2f}%')\n",
        "print(f'  Macro F1      : {metrics[\"macro avg\"][\"f1-score\"]*100:.2f}%')\n",
        "print(f'  Weighted F1   : {metrics[\"weighted avg\"][\"f1-score\"]*100:.2f}%')\n",
        "print('=' * 55)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
