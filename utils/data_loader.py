"""
data_loader.py
--------------
Utilities to load and merge CSV datasets from the CIC-DDoS2019
and CIC-IDS2017 directories into a single unified DataFrame.
"""

import os
import glob
from pathlib import Path
from typing import Optional

import pandas as pd
from tqdm import tqdm

# ---------------------------------------------------------------------------
# Columns that carry no predictive signal for the model (identity / metadata)
# ---------------------------------------------------------------------------
_DEFAULT_DROP_COLS = ["src_ip", "dst_ip", "timestamp", "Timestamp"]
_LABEL_COL = "Label"


def load_dataset(
    directory: str,
    label_col: str = _LABEL_COL,
    drop_cols: Optional[list] = None,
    sample_frac: Optional[float] = None,
    random_state: int = 42,
    verbose: bool = True,
) -> pd.DataFrame:
    """Load all CSV files from *directory* and return a concatenated DataFrame.

    Parameters
    ----------
    directory : str
        Path to the dataset directory (e.g. ``data/CIC-DDoS2019``).
    label_col : str
        Name of the target label column.
    drop_cols : list, optional
        Columns to drop before returning (metadata / identifier columns).
        Defaults to src_ip, dst_ip, timestamp.
    sample_frac : float, optional
        If set (0 < frac ≤ 1) each file is randomly sampled to this fraction
        before concatenation. Useful for quick development runs.
    random_state : int
        Random seed for reproducibility when sampling.
    verbose : bool
        Print progress information.

    Returns
    -------
    pd.DataFrame
        Combined DataFrame from all CSVs in the directory, or an empty
        DataFrame if no CSV files are found.
    """
    if drop_cols is None:
        drop_cols = _DEFAULT_DROP_COLS

    csv_files = sorted(glob.glob(os.path.join(directory, "*.csv")))

    if not csv_files:
        if verbose:
            print(f"[data_loader] No CSV files found in: {directory}")
        return pd.DataFrame()

    frames = []
    for path in tqdm(csv_files, desc=f"Loading {Path(directory).name}", disable=not verbose):
        df = pd.read_csv(path, low_memory=False)

        # Rename columns: strip whitespace that sometimes appears in headers
        df.columns = df.columns.str.strip()

        if sample_frac is not None:
            df = df.sample(frac=sample_frac, random_state=random_state)

        frames.append(df)

    combined = pd.concat(frames, ignore_index=True)

    # Drop metadata columns that are present in the data
    existing_drop = [c for c in drop_cols if c in combined.columns]
    if existing_drop and verbose:
        print(f"[data_loader] Dropping metadata columns: {existing_drop}")
    combined.drop(columns=existing_drop, inplace=True, errors="ignore")

    if verbose:
        print(
            f"[data_loader] Loaded {len(csv_files)} file(s) from '{Path(directory).name}' "
            f"→ {combined.shape[0]:,} rows × {combined.shape[1]} cols"
        )
        if label_col in combined.columns:
            print(f"[data_loader] Class distribution:\n{combined[label_col].value_counts().to_string()}\n")

    return combined


def load_all_datasets(
    ddos2019_dir: str,
    ids2017_dir: str,
    label_col: str = _LABEL_COL,
    drop_cols: Optional[list] = None,
    sample_frac: Optional[float] = None,
    random_state: int = 42,
    verbose: bool = True,
) -> pd.DataFrame:
    """Load both CIC-DDoS2019 and CIC-IDS2017 datasets and combine them.

    If a directory is empty or missing, it is skipped silently.

    Parameters
    ----------
    ddos2019_dir : str
        Path to the CIC-DDoS2019 CSV directory.
    ids2017_dir : str
        Path to the CIC-IDS2017 CSV directory.
    label_col : str
        Name of the target label column (must be present in both datasets).
    drop_cols : list, optional
        Metadata columns to drop before returning.
    sample_frac : float, optional
        Fraction of each file to sample (for development runs).
    random_state : int
        Random seed.
    verbose : bool
        Print progress information.

    Returns
    -------
    pd.DataFrame
        Combined DataFrame from all available datasets.
    """
    frames = []

    for directory in [ddos2019_dir, ids2017_dir]:
        df = load_dataset(
            directory,
            label_col=label_col,
            drop_cols=drop_cols,
            sample_frac=sample_frac,
            random_state=random_state,
            verbose=verbose,
        )
        if not df.empty:
            frames.append(df)

    if not frames:
        raise ValueError("No data could be loaded from any of the specified directories.")

    combined = pd.concat(frames, ignore_index=True)

    if verbose:
        print(
            f"\n[data_loader] Combined dataset: {combined.shape[0]:,} rows × {combined.shape[1]} cols"
        )
        if label_col in combined.columns:
            print(f"[data_loader] Overall class distribution:\n{combined[label_col].value_counts().to_string()}\n")

    return combined
